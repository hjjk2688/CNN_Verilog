# FPGA 기반 터치패드 연동 실시간 CNN 가속기 최종 프로젝트 보고서

## 1. 서론 (Introduction)

### 1.1 연구 배경 및 목적

최근 인공지능 기술이 엣지(Edge) 디바이스로 확산됨에 따라, 제한된 자원 환경에서 AI 연산을 효율적으로 수행하는 것이 핵심 과제로 부상했습니다. 일반적인 CPU는 순차 처리 방식의 한계로 인해 실시간 AI 추론에 요구되는 높은 성능과 전력 효율을 동시에 만족시키기 어렵습니다. 이에 대한 효과적인 해결책으로, 하드웨어 수준에서 병렬 처리가 가능한 FPGA(Field-Programmable Gate Array)가 주목받고 있습니다.

본 프로젝트는 Zynq-7000 SoC 플랫폼을 활용하여 **터치패드를 통해 입력된 손글씨 데이터를 실시간으로 인식하는 고성능 CNN 가속기를 설계하고 구현**하는 것을 목표로 합니다. 이를 통해 AI 모델링(Python)부터 RTL 설계(Verilog), 임베디드 시스템 통합(C)까지 아우르는 End-to-End 시스템 구축 역량을 확보하고자 합니다.

### 1.2 시스템 개요

구현된 시스템의 전체적인 데이터 흐름은 다음과 같은 3단계로 요약할 수 있습니다.

1. **입력 (Input)**: 사용자가 터치패드에 손글씨를 입력합니다.
2. **연산 (Computation)**: FPGA에 구현된 CNN 가속기가 입력 데이터를 받아 실시간으로 추론 연산을 수행합니다.
3. **출력 (Output)**: 인식된 숫자 결과를 VGA 디스플레이를 통해 시각적으로 표시합니다.

본 보고서는 제안하는 시스템의 아키텍처부터 하드웨어 상세 설계, 구현 결과 및 성과에 이르기까지 프로젝트 전반의 기술적 내용을 체계적으로 기술할 것입니다. 다음 장에서는 시스템의 근간을 이루는 전체 아키텍처에 대해 상세히 설명하겠습니다.

---

## 2. 전체 시스템 구조 (Overall System Architecture)

본 시스템은 Zynq SoC의 핵심 특징인 PS(Processing System)와 PL(Programmable Logic)을 활용한 하드웨어/소프트웨어 공동 설계(Co-design)를 기반으로 합니다. PS의 ARM 코어가 전체 시스템 제어와 데이터 준비를 담당하고, PL의 FPGA 영역에는 맞춤형으로 설계된 CNN 가속기가 배치되어 실제 연산을 수행합니다. 이러한 이기종 컴퓨팅 아키텍처는 소프트웨어의 유연성과 하드웨어의 압도적인 성능을 동시에 확보하는 최적의 솔루션입니다.

### 2.1. 데이터 흐름 및 시스템 구성도

시스템의 데이터는 명확하게 정의된 파이프라인을 따라 처리됩니다. 각 단계는 AXI(Advanced eXtensible Interface) 프로토콜을 통해 유기적으로 연결되어 고속 데이터 전송을 보장합니다.

1. **입력 (Touchpad)**: 터치패드로부터 수집된 손글씨 좌표 데이터는 PS에서 28x28 이미지로 전처리된 후 DDR 메모리에 저장됩니다.
2. **데이터 전송 (AXI DMA)**: PS의 제어에 따라 AXI DMA가 DDR 메모리에서 이미지 데이터를 읽어 AXI-Stream 프로토콜 기반의 데이터 패킷으로 변환하여 PL 영역으로 고속 전송합니다.
3. **CNN 가속 (FPGA PL)**: PL에 위치한 CNN 가속기 IP는 스트림 데이터를 실시간으로 수신하여 합성곱, 풀링, 완전 연결 등 모든 추론 연산을 하드웨어적으로 처리합니다.
4. **결과 전달 및 출력 제어 (Zynq PS)**: CNN 가속기에서 최종 판별된 숫자(0~9) 결과는 PS로 전달됩니다. PS는 이 결과를 바탕으로 VGA 디스플레이에 출력할 화면을 제어합니다.
5. **시각화 (VGA IP)**: PS의 제어에 따라 AXI VDMA가 DDR 메모리의 프레임 버퍼를 읽어 VGA 컨트롤러로 전송하고, 최종적으로 모니터에 결과가 실시간으로 표시됩니다.

시스템을 구성하는 주요 IP(Intellectual Property)와 그 역할은 아래 표와 같습니다.

| 구성 요소 | 역할 | 적용 인터페이스 |
|----------|------|----------------|
| Zynq PS | 시스템 전체 제어, 데이터 전처리, IP 구동 및 결과 처리 | AXI4-Lite (제어), AXI4-MM (데이터) |
| AXI DMA | DDR 메모리와 PL IP 간의 고속 메모리-스트림 데이터 전송 | AXI4-Memory Map / AXI4-Stream |
| AXI VDMA | DDR 내 비디오 프레임 버퍼를 관리하고 VGA 컨트롤러로 전송 | AXI4-Memory Map / AXI4-Stream |
| Data FIFO | 클럭 도메인 동기화 및 데이터 속도 차이를 완충하는 버퍼 | AXI4-Stream (비동기 모드) |
| Width Converter | 데이터 버스 폭 정렬 (AXI 32bit → CNN IP 8bit) | AXI4-Stream |
| CNN 가속기 (cnn_top) | CNN 추론 연산의 핵심을 수행하는 맞춤형 하드웨어 로직 | AXI4-Stream |
| VGA Controller | AXI-Stream 데이터를 받아 VGA 타이밍 신호를 생성 및 출력 | AXI4-Stream → Physical I/O |

### 2.2 AI 모델 아키텍처 (Model Structure)

본 가속기에 구현된 CNN은 두 개의 합성곱/풀링 블록과 하나의 완전 연결 계층으로 구성된 경량화 모델입니다. 입력 이미지가 각 계층을 통과하며 데이터의 형태(Shape)가 변화하는 과정은 다음과 같습니다.

| 단계 | 레이어 (Layer) | 입력 크기 (Input Shape) | 커널/필터 정보 | 출력 크기 (Output Shape) |
| --- | --- | --- | --- | --- |
| **0** | **Input Image** | 28 x 28 x 1 | - | 28 x 28 x 1 |
| **1** | **Conv2D 1** | 28 x 28 x 1 | 5x5 Kernel, 3 Filters, ReLU | 24 x 24 x 3 |
| **2** | **MaxPooling2D 1** | 24 x 24 x 3 | 2x2 Pooling | 12 x 12 x 3 |
| **3** | **Conv2D 2** | 12 x 12 x 3 | 5x5 Kernel, 3 Filters, ReLU | 8 x 8 x 3 |
| **4** | **MaxPooling2D 2** | 8 x 8 x 3 | 2x2 Pooling | 4 x 4 x 3 |
| **5** | **Flatten** | 4 x 4 x 3 | - | 48 |
| **6** | **Dense (FC)** | 48 | 10 Units, Softmax | 10 (Class 0~9) |

---

## 3. 하드웨어 설계 (Hardware Design)

### 3.1 클럭 아키텍처 및 CDC 설계

시스템의 안정성과 성능을 최적화하기 위해, 각 IP의 요구 사항에 맞춰 3개의 독립적인 클럭 도메인을 구성했습니다.

| 클럭 도메인 | 주파수 | 적용 모듈 | 설계 근거 |
|-----------|--------|----------|----------|
| 시스템 클럭 | 50 MHz | Zynq PS, DMA, VDMA, AXI Interconnect | DDR 메모리 인터페이스와의 고속 데이터 전송 효율 최적화 |
| VGA 클럭 | 25.175 MHz | VGA Controller | VGA 640×480@60Hz 해상도의 표준 비디오 타이밍 규격 준수 |
| CNN 클럭 | 10 MHz | CNN 가속기 | 복잡한 이진 덧셈 트리(Binary Adder Tree) 파이프라인이 타이밍 위반 없이 단일 클럭 내에 완료되도록 보장하고, 설계 초기 단계에서 라우팅 복잡도를 완화하며 디버깅 용이성을 증대 |

서로 다른 클럭 속도로 동작하는 도메인 간의 데이터 전송에서 발생할 수 있는 CDC(Clock Domain Crossing) 문제는 **비동기 FIFO(Asynchronous FIFO)** 를 통해 해결했습니다. 시스템에 배치된 AXI4-Stream Data FIFO IP를 Independent Clock 모드로 설정하여, 50MHz의 시스템 클럭과 10MHz의 CNN 클럭 간의 속도 차이를 안전하게 완충하고 데이터 무결성을 완벽하게 보장합니다.

이처럼 견고하게 설계된 시스템 아키텍처는 본 프로젝트의 핵심인 CNN 가속기가 최대의 성능을 발휘할 수 있는 안정적인 기반을 제공합니다. 다음 장에서는 가속기의 내부 상세 설계에 대해 심층적으로 분석하겠습니다.

---

## 4. CNN 가속기 상세 설계 (Detailed Design)

본 장에서는 CNN 가속기의 핵심 성능을 결정하는 RTL(Verilog) 수준의 설계 최적화 기법들을 다룹니다. 단순히 기능을 구현하는 것을 넘어, Verilog 코드가 어떻게 특정 하드웨어 동작(파이프라인, 병렬 처리, 자원 재사용)을 구현했는지 코드 스니펫과 함께 심층 분석함으로써, 하드웨어 아키텍처에 대한 깊이 있는 이해를 제시합니다.

### 4.1 시스템 인지 양자화(하드웨어 친화적 양자화): 고정 스케일링 (Fixed Scaling)

FPGA의 제한된 리소스를 효율적으로 사용하기 위해 복잡한 부동소수점 연산을 배제하고 INT8 고정소수점 연산을 채택했습니다. 특히, 본 시스템의 입력 데이터가 터치패드를 통해 생성되는 0 또는 1의 이진(Binary) 데이터라는 특성에 착안하여, 하드웨어 복잡도를 획기적으로 낮추는 고정 스케일링(Fixed Scaling) 기법을 적용했습니다.

이 기법의 핵심은 signed 8-bit 정수(-128 ~ +127)의 표현 범위를 최대로 활용하는 것입니다. 스케일링 계수로 표현 가능한 가장 큰 양수인 127을 선택함으로써, 정수 변환 과정에서의 정보 손실을 최소화하고 정밀도를 극대화했습니다.

그러나 이 방식은 연산 과정에서 '스케일 폭발(Scale Explosion)' 문제를 야기합니다. 127배로 스케일링된 두 INT8 값이 곱해지면, 그 결과는 $127^2 \approx 2^{14}$배로 스케일이 커지게 됩니다. 다음 계층으로 전달하기 전, 이 값을 다시 128($2^7$)배 수준으로 낮추는 리스케일링(Re-scaling) 과정이 필수적입니다. 이때, 하드웨어에서 매우 비싼 연산인 나눗셈기를 사용하는 대신, 우측 비트 시프트(`>>> 7`) 연산으로 대체하여 DSP(Digital Signal Processing) 자원을 절약하고 타이밍 마진을 확보하는 최적화를 수행했습니다


1. **스케일 증폭:** 입력($2^7$)과 가중치($2^7$)의 곱셈 연산으로 인해 출력값의 스케일이 **$ 2^{14} $ 배**로 과도하게 커집니다 ($2^7 \times 2^7 = 2^{14}$).
2. **스케일 복원:** 다음 계층이 요구하는 $2^7$ 스케일로 정규화하기 위해, 연산 결과를 $2^7$ (128)로 나누어야 합니다 ($2^{14} \div 2^7 = 2^7$).
3. **하드웨어 최적화:** 복잡한 나눗셈기 대신 우측 비트 시프트(`>>> 7`) 연산을 사용하여 동일한 수학적 결과를 얻으면서 DSP 자원을 절약했습니다.

```verilog
// [코드 분석: conv2_calc_3.v]
// 나눗셈(/128) 대신 우측 시프트(>>>7)를 사용하여 DSP 블록을 절약하고 타이밍을 확보함
conv_out_calc <= ($signed(final_sum_s7) >>> 7) + 8'shcf;

```

마지막으로, 본 프로젝트는 최종 결과로 '어떤 숫자가 가장 높은 점수를 가졌는가(Argmax)'만을 요구합니다. 모든 출력값에 동일한 스케일이 적용되어 있으므로 값들의 순위(대소 관계)는 변하지 않습니다. 따라서 자원을 소모하는 역양자화(De-quantization) 과정을 생략하여 추가적인 하드웨어 절약 효과를 얻었습니다.


### 4.2 중앙 제어 로직 (FSM Design)

가속기 전체의 데이터 흐름과 상태를 관장하기 위해 5단계 FSM(Finite State Machine)을 설계했습니다.

* **상태 정의:** `IDLE` → `RUN_CNN` (데이터 수신) → `PADDING` (플러싱) → `WAIT_DONE` → `RESULT`

* **S_IDLE**: 시스템 초기화 및 시작(start_sw) 신호 대기
  * start_sw 입력 시 → S_RUN_CNN
* **S_RUN_CNN**: AXI-Stream 인터페이스를 통해 외부로부터 이미지 데이터를 수신
  * 마지막 데이터(tlast) 수신 시 → S_PADDING
* **S_PADDING**: 파이프라인 플러싱(Pipeline Flushing) 수행
  * 일정 시간(padding_cnt) 경과 후 → S_WAIT_DONE
* **S_WAIT_DONE**: 완전 연결 계층의 연산 완료 신호 대기
  * fc_valid 신호 감지 시 → S_RESULT
* **S_RESULT**: 최종 추론 결과를 출력하고 S_IDLE로 복귀 준비
  * start_sw 해제 시 → S_IDLE

본 가속기는 깊은 파이프라인 구조를 가지므로, 외부 데이터 입력이 끝난 후에도 내부 레지스터에는 아직 처리 중인 데이터가 남아있습니다. 이러한 데이터를 유실 없이 끝까지 처리하기 위한 파이프라인 플러싱 전략이 필수적입니다. S_PADDING 상태는 바로 이 문제를 해결하기 위해 고안되었습니다. 이 상태에서는 외부 데이터 입력을 차단하고 '0'을 강제로 주입하여, 파이프라인 내부에 남아있는 유효 데이터를 끝까지 밀어냅니다.

```verilog
// cnn_top.v
localparam S_IDLE = 3'd0, S_RUN_CNN = 3'd1, S_PADDING = 3'd2, ...;

// S_PADDING 상태일 때 외부 데이터 대신 0(검정색)을 강제 주입
assign cnn_data_in = (state == S_PADDING) ? 8'd0 : s_axis_tdata;

// padding_cnt를 이용해 일정 시간 동안 플러싱을 유지
if (state == S_PADDING && padding_cnt > 2000) state <= S_WAIT_DONE;
```

### 4.3 합성곱 연산 계층: 극단적 병렬화

합성곱 계층은 전체 연산의 병목 구간이므로 **'속도 최우선'** 설계를 적용했습니다.

#### 4.3.1 라인 버퍼 (Line Buffer)

하드웨어 Shift Register Array를 구현한 것으로, 1차원 픽셀 스트림을 입력받으면서도 매 클럭 2차원 공간 정보(5x5 윈도우)를 유지할 수 있게 해주는 스트림 처리의 핵심 기법입니다. 전체 이미지(28x28=784 픽셀)를 저장하는 대신, 연산에 필요한 단 5줄의 데이터(5x28=140 픽셀)만 저장하여 메모리 사용량을 절감했습니다.

```verilog
// conv_buf 모듈
// 매 클럭, 데이터가 한 줄씩 위로 이동하며 저장됨 (Data Reuse)
line4_regs[col_cnt] <= line3_regs[col_cnt];
line3_regs[col_cnt] <= line2_regs[col_cnt];
line2_regs[col_cnt] <= line1_regs[col_cnt];
line1_regs[col_cnt] <= data_in;
```

`conv_buf` 모듈은 스트리밍 입력 데이터로부터 2D 합성곱에 필요한 5×5 윈도우를 생성하는 라인 버퍼를 구현한다.

**동작 원리:**
1. 이미지는 1차원 스트림으로 입력되지만, 합성곱은 2D 공간 정보가 필요하다.
2. 4개의 라인 레지스터(`line1_regs` ~ `line4_regs`)에 이전 행들을 저장한다.
3. 새로운 픽셀이 입력될 때마다 5×5=25개의 픽셀을 동시에 출력(`data_out_0` ~ `data_out_24`)하여 계산 모듈로 전달한다.

이를 통해 **매 클럭마다 하나의 출력 픽셀**을 생성하는 고속 파이프라인을 구현한다.


#### 4.2.2 커널 병렬성(Kernel Parallelism)

conv_calc 설계는 **완전 병렬 커널 연산(Fully Parallel Kernel Computation)** 방식을 채택하였다.

**구현 메커니즘:**
```verilog
// conv1_calc.v 핵심 로직
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p_s0[i]) * get_w1(i);
    product2_s1[i] <= $signed(p_s0[i]) * get_w2(i);
    product3_s1[i] <= $signed(p_s0[i]) * get_w3(i);
end
```

위 코드에서 `for`문은 소프트웨어의 순차 반복이 아닌, **25개의 곱셈기를 물리적으로 병렬 배치**하는 하드웨어 기술 방식이다. 따라서 5×5 커널의 모든 원소와 입력 윈도우 간의 곱셈이 **단일 클럭 사이클 내에 동시 실행** 된다.


#### 4.3.3 이진 덧셈 트리 (Binary Adder Tree)

25개의 곱셈 결과를 순차적으로 더하는 대신, 토너먼트 방식의 트리 구조로 더하여 Critical Path를 단축시켰습니다. 이는 낮은 클럭 속도에서도 높은 처리량을 보장합니다.

```
Stage 1: 25개의 곱셈 결과
Stage 2: 13개로 축소 (2개씩 덧셈, 1개 직접 전달)
Stage 3: 7개로 축소
Stage 4: 4개로 축소
Stage 5: 2개로 축소
Stage 6: 최종 합계 1개
```

```verilog
// [코드 분석: conv1_calc.v]
// 25개의 곱셈기가 하드웨어적으로 병렬 생성되어 1클럭에 동시 연산 수행
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p_s0[i]) * get_w1(i);
end
// 이후 Stage 2~6에서 계층적 덧셈 수행 (Pipeline Registers)
for (i=0; i<12; i=i+1) sum1_s2[i] <= product1_s1[2*i] + product1_s1[2*i+1];

```

#### 4.2.4 다채널 처리(Multi-Channel Processing)

`conv2_calc_1` 모듈은 입력 채널이 3개로 증가한 2번째 합성곱 레이어를 처리한다:

```verilog
// 채널별 독립 계산
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p1_s0[i]) * get_w1(i); // 채널 1
    product2_s1[i] <= $signed(p2_s0[i]) * get_w2(i); // 채널 2
    product3_s1[i] <= $signed(p3_s0[i]) * get_w3(i); // 채널 3
end

// 최종 단계에서 채널 결과 합산
final_sum_s7 <= sum1_s6 + sum2_s6 + sum3_s6;
```

**결과:** 총 75개(25×3)의 곱셈기가 병렬로 동작하며, 이는 **3D 합성곱(C_in × K_h × K_w)의 완전 병렬화** 를 의미한다.

### 4.4 완전 연결 계층: 자원 효율화

합성곱 계층이 속도 극대화를 위해 병렬 구조를 채택한 것과 달리, 완전 연결 계층(FC Layer)은 의도적인 아키텍처 트레이드오프의 결과로 자원 효율성을 극대화하기 위해 시분할(Time-Sharing) 연산 구조로 설계되었습니다. FC 계층은 합성곱 계층보다 연산량이 적으므로, 단 하나의 MAC(Multiply-Accumulate) 연산기를 FSM 제어를 통해 재사용하여 DSP와 같은 고비용 자원을 절약하고, 이를 성능이 더 중요한 합성곱 계층에 할당하는 전략적 선택을 했습니다.

또한 시스템 안정성 확보를 위해, input_buffer에 데이터를 쓰는 로직을 제어 FSM이 포함된 always 블록과 분리했습니다. 이는 리셋 신호가 제어 로직에만 영향을 미치고 데이터 저장 로직에는 영향을 주지 않도록 하여, 의도치 않은 메모리 오염(corruption)을 방지하는 효과적인 FPGA 설계 기법입니다.

```verilog
// [코드 분석: fully_connected.txt]
// 데이터 저장 로직을 별도 always 블록으로 분리하여 리셋 오동작 방지
always @(posedge clk) begin
    if (valid_in) begin
        // ... (버퍼링 로직)
        input_buffer[buffer_cnt * 3] <= $signed(data_in_1);
    end
end

```
---

## 4.5 VGA 디스플레이 컨트롤러 설계 (VGA Display Controller)

VGA 컨트롤러는 VDMA로부터 스트리밍되는 영상 데이터를 받아 표준 모니터 타이밍(Hsync, Vsync)에 맞춰 픽셀을 출력하는 역할을 합니다. 본 프로젝트에서는 **640x480 @ 60Hz** 해상도를 지원하며, AXI4-Stream 프로토콜과 VGA 타이밍 간의 비동기 문제를 해결하기 위해 **초기 프레임 동기화(Frame Synchronization) 로직**을 적용했습니다.

### 4.5.1 해상도 및 타이밍 규격

표준 VGA 신호 규격을 준수하기 위해 25MHz 픽셀 클럭을 기반으로 수평(Horizontal) 및 수직(Vertical) 타이밍 파라미터를 설정하였습니다.

```verilog
// [코드 분석: vga_controller.v]
// 640x480 @ 60Hz 표준 타이밍 파라미터 정의
localparam H_VISIBLE = 640;
localparam H_TOTAL   = 800; // Visible + Front + Sync + Back
localparam V_VISIBLE = 480;
localparam V_TOTAL   = 525;

```

### 4.5.2 프레임 동기화 및 AXI 핸드셰이킹

VDMA는 메모리 상황에 따라 버스트로 데이터를 보내지만, VGA는 엄격한 실시간 타이밍을 요구합니다. 특히 시스템 초기 구동 시, 데이터 스트림의 시작점과 VGA 카운터의 (0,0) 시점이 어긋나는 현상(Image Rolling)을 방지하기 위해 **강제 정렬 로직**을 구현했습니다.

* **동기화 전략:** 시스템 리셋 후 아직 동기화되지 않은 상태(`!system_synced`)에서 첫 번째 유효 데이터(`tvalid`)가 도착하면, 그 즉시 `h_cnt`와 `v_cnt`를 0으로 강제 초기화하여 프레임의 시작점을 맞춥니다.

```verilog
// [코드 분석: vga_controller.v]
// 시스템 켜진 후 첫 데이터(Valid)가 도착했을 때 카운터 강제 리셋 (Sync 맞춤)
if (!system_synced && s_axis_tvalid && s_axis_tready) begin
    h_cnt <= 0;          // 좌표를 무조건 0,0으로 강제 초기화
    v_cnt <= 0;
    system_synced <= 1;  // 정렬 완료 플래그 설정 (이후에는 일반 카운팅 동작)
end

```

또한, VGA가 실제로 화면을 출력하는 유효 구간(`active_area`)에서만 데이터를 요청(`tready = 1`)하는 백프레셔(Backpressure) 메커니즘을 통해 데이터 흐름을 제어합니다.

```verilog
// [코드 분석: vga_controller.v]
// 화면 출력 구간(Active Area)이거나, 초기 싱크를 맞추는 중일 때만 Ready 신호 출력
assign s_axis_tready = active_area || (!system_synced && s_axis_tvalid);

```

---

## 5. 소프트웨어 설계 및 학습 (Software & Training)

터치패드 입력 데이터는 일반적인 MNIST 이미지와 달리 **노이즈가 없고, 경계가 뚜렷하며(계단 현상), 압력 감지가 불가능한 특성** 을 가집니다. 이를 모델 학습 단계에 반영하기 위해 특수한 데이터 증강(Augmentation) 기법을 적용했습니다.

### 5.1 터치패드 패턴 모방 함수 (`add_touchpad_noise`)

학습 데이터에 실제 하드웨어 입력 환경과 유사한 노이즈를 주입하여 모델의 견고성을 높였습니다.

1. **강제 이진화 (Thresholding):**
* 터치패드의 '누름/안 누름' 특성을 반영하여 0.3 이상의 값은 무조건 1.0으로 변환합니다.
* 이를 통해 회색조(Anti-aliasing)를 제거하고 **계단 현상(Aliasing)** 을 인위적으로 생성합니다.


```python
binary_img = np.where(img > 0.3, 1.0, 0.0).astype(np.float32)

```


2. **획 두께 변형 (Random Morphology):**
* 손가락의 압력 차이를 시뮬레이션하기 위해 50% 확률로 **팽창(Dilate)** 또는 **침식(Erode)** 연산을 수행합니다.
* 2x2 커널을 사용하여 미세한 두께 변화를 유도합니다.


```python
if np.random.rand() > 0.5:
    binary_img = cv2.dilate(binary_img, kernel, iterations=1) # 꾹 눌렀을 때
else:
    binary_img = cv2.erode(binary_img, kernel, iterations=1)  # 살짝 닿았을 때

```


3. **가장자리 노이즈 (Edge Jitter):**
* 터치 센서의 전기적 노이즈를 반영하기 위해 획의 경계선에 미세한 무작위 노이즈를 추가합니다.


```python
noise = np.random.normal(0, 0.1, binary_img.shape)
noisy_img = binary_img + noise

```


### 5.2 가중치 추출 및 ROM 변환

학습이 완료된 모델(`my_cnn_wild_model_hwc_v2.keras`)의 가중치(Weights)와 편향(Bias)을 추출하고, 이를 `quantize_weights` 함수를 통해 INT8로 양자화한 뒤 Verilog `function` (ROM 형태)으로 변환하여 FPGA 내부에 임베딩하였습니다.

---

## 6. 구현 결과 및 성과 (Results)

본 장에서는 설계된 CNN 가속기의 성능을 정량적으로 평가하고, 동일 알고리즘을 PC 환경에서 실행했을 때와 비교 분석합니다. 더 나아가, 프로젝트를 통해 달성한 기술적 성과와 그 의의를 종합적으로 논의합니다.

### 6.1 성능 분석 (PC vs FPGA)

구현된 FPGA 가속기의 성능을 검증하기 위해, 동일한 CNN 알고리즘을 PC(CPU) 환경에서 최적화하여 실행한 결과와 추론 시간을 비교했습니다.

| 구분 | PC (Software) | FPGA (Hardware) | 비고 |
| --- | --- | --- | --- |
| **플랫폼** | Intel/AMD CPU (GHz급) | Zynq-7000 (10 MHz) |  |
| **연산 방식** | 순차 처리 (Sequential) | **완전 병렬 처리 (Fully Parallel)** |  |
| **총 추론 시간** | 0.5374 ms | **0.2787 ms** | **약 1.93배 가속** |
| **처리 속도** | ~1,860 FPS | **~3,588 FPS** |  |

분석 결과, 10MHz라는 매우 낮은 클럭으로 동작하는 FPGA 가속기가 수 GHz의 고성능 PC CPU보다 약 1.93배 빠른 성능을 기록했습니다. 이러한 결과의 핵심 원인은 다음과 같습니다.

1. **압도적인 사이클 효율성**: PC는 운영체제 오버헤드, 명령어 디코딩 등으로 인해 실제 연산 외에 많은 클럭을 소모합니다. 반면, 본 가속기는 데이터 처리 파이프라인이 고도로 최적화되어, 단 2,787 클럭 사이클 만에 하나의 이미지에 대한 모든 추론 연산을 완료합니다.

2. **병렬 처리의 힘**: CPU가 기본적으로 순차 처리에 의존하는 반면, FPGA 가속기는 낮은 클럭 속도의 한계를 극복하기 위해 대규모 병렬 구조를 채택했습니다. 특히 Conv2 계층에서 하나의 출력 특징맵을 계산하기 위해 3개의 입력 채널에 대한 5x5 커널 연산(25 x 3 = 75개의 곱셈)이 병렬로 처리되면서 전체 처리 시간을 획기적으로 단축시켰습니다.

3. **결정론적 레이턴시 (Deterministic Latency)**: CPU는 OS 스케줄링 및 캐시 상태에 따라 실행 시간이 미세하게 변동(Jitter)될 수 있지만, FPGA 가속기는 입력이 시작되면 언제나 정확히 2,787 사이클 후에 결과가 출력됨을 보장합니다. 이는 실시간성이 중요한 임베디드 시스템에서 결정적인 장점입니다.


### 6.2 리소스 사용률

효율적인 아키텍처 설계(고정 스케일링, 시분할 연산)를 통해 Zynq-7000의 제한된 자원 내에서 성공적으로 구현되었습니다.

###  FPGA 리소스 사용량 및 비율 (Resource Utilization)

| 리소스 (Resource) | 사용량 (Utilization) | 전체 가용량 (Available) | 사용률 (Utilization %) | 비고 (Description) |
| --- | --- | --- | --- | --- |
| **LUT** | 9,569 | 53,200 | **17.99%** | 로직 구현 (Logic) |
| **LUTRAM** | 413 | 17,400 | **2.37%** | 분산 메모리 (Distributed RAM) |
| **FF** | 13,726 | 106,400 | **12.90%** | 플립플롭 (Flip-Flop) |
| **BRAM** | 5 | 140 | **3.57%** | 블록 램 (Block RAM) |
| **DSP** | 181 | 220 | **82.27%** | 디지털 신호 처리 (DSP48) |
| **IO** | 32 | 125 | **25.60%** | 입출력 포트 (I/O) |
| **BUFG** | 3 | 32 | **9.38%** | 글로벌 버퍼 (Global Buffer) |

---


1. **DSP 사용률 (82.27%):**
* 합성곱(Convolution) 연산의 핵심인 **병렬 곱셈기** 구현에 DSP 자원의 약 82%를 투입하였습니다. 이는 **'속도 중심의 병렬 처리(Parallel Processing)'** 전략이 하드웨어 리소스를 적극적으로 활용하여 구현되었음을 보여줍니다.


2. **LUT 및 FF 사용률 (약 13~18%):**
* 복잡한 파이프라인 제어 로직과 상태 머신(FSM)을 구현했음에도 불구하고, 로직 자원은 비교적 여유롭게 사용되었습니다. 이는 향후 기능 확장이나 추가적인 최적화를 위한 공간이 충분함을 의미합니다.


3. **BRAM 효율성 (3.57%):**
* **라인 버퍼(Line Buffer)** 기술을 사용하여 전체 이미지를 저장하지 않고 필요한 행(Row)만 저장했기 때문에, 메모리 자원 사용량을 극도로 절약할 수 있었습니다.


### 6.3. 핵심 성과 및 의의

본 프로젝트를 통해 달성한 핵심 기술적 성과는 다음과 같습니다.

1. **Full-Stack 시스템 설계 역량 확보**: AI 모델링(SW)부터 RTL 하드웨어 설계(HW), 임베디드 시스템 통합에 이르기까지 AI 가속기 개발의 전 과정을 직접 구축하며 영역 간 상호작용에 대한 깊이 있는 이해를 확보했습니다.

2. **하드웨어 최적화 사고방식 체득**: 성능이 병목인 합성곱 계층에는 **'속도 중심의 병렬 처리'** 를, 상대적으로 연산량이 적은 완전 연결 계층에는 **'자원 중심의 직렬 처리'** 를 적용하는 등, 목표에 따라 최적의 아키텍처를 선택하는 전략적 트레이드오프 분석 능력을 체득했습니다.

3. **SoC 표준 인터페이스 마스터**: 산업 표준인 AXI-Stream 프로토콜의 valid/ready 핸드셰이크 메커니즘을 완벽히 이해하고, ILA(Integrated Logic Analyzer)를 활용하여 실제 하드웨어에서 발생하는 타이밍 문제를 실시간으로 디버깅하는 실무 능력을 확보했습니다.

---

## 7. 결론 (Conclusion)

본 프로젝트는 Zynq-7000 SoC 플랫폼을 활용하여 터치패드 입력부터 CNN 연산, VGA 출력까지 이어지는 End-to-End 실시간 AI 추론 시스템을 성공적으로 구현했습니다. 

이는 10MHz의 낮은 클럭으로 동작하는 맞춤형 하드웨어 패브릭이 극단적인 병렬 처리와 사이클 효율성을 통해 수 GHz의 범용 프로세서를 능가할 수 있음을 보여주는 아키텍처 수준 최적화 의 성공적인 사례입니다.

특히, 데이터 재사용을 극대화한 라인 버퍼, Critical Path를 단축시킨 이진 덧셈 트리, 그리고 성능과 자원 간의 균형을 맞춘 병렬/직렬 아키텍처의 전략적 선택은 단순히 하나의 프로젝트 완성을 넘어, 

향후 더 복잡하고 깊은 신경망을 FPGA에 구현하기 위한 견고한 **'아키텍처적 청사진(Architectural Blueprint)'** 을 마련했다는 점에서 큰 의의를 가집니다.
