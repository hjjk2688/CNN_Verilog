# FPGA 기반 터치패드 연동 실시간 CNN 가속기 최종 프로젝트 보고서

## 1. 서론 (Introduction)

### 1.1 연구 배경 및 목적

최근 인공지능 기술이 엣지(Edge) 디바이스로 확산됨에 따라, 제한된 자원 환경에서 AI 연산을 효율적으로 수행하는 것이 핵심 과제로 부상했습니다. 일반적인 CPU는 순차 처리 방식의 한계로 인해 실시간 AI 추론에 요구되는 높은 성능과 전력 효율을 동시에 만족시키기 어렵습니다. 이에 대한 효과적인 해결책으로, 하드웨어 수준에서 병렬 처리가 가능한 FPGA(Field-Programmable Gate Array)가 주목받고 있습니다.

본 프로젝트는 Zynq-7000 SoC 플랫폼을 활용하여 **터치패드를 통해 입력된 손글씨 데이터를 실시간으로 인식하는 고성능 CNN 가속기를 설계하고 구현**하는 것을 목표로 합니다. 이를 통해 AI 모델링(Python)부터 RTL 설계(Verilog), 임베디드 시스템 통합(C)까지 아우르는 End-to-End 시스템 구축 역량을 확보하고자 합니다.

### 1.2 시스템 개요

구현된 시스템의 전체적인 데이터 흐름은 다음과 같은 3단계로 요약할 수 있습니다.

1. **입력 (Input)**: 사용자가 터치패드에 손글씨를 입력합니다.
2. **연산 (Computation)**: FPGA에 구현된 CNN 가속기가 입력 데이터를 받아 실시간으로 추론 연산을 수행합니다.
3. **출력 (Output)**: 인식된 숫자 결과를 VGA 디스플레이를 통해 시각적으로 표시합니다.

본 보고서는 제안하는 시스템의 아키텍처부터 하드웨어 상세 설계, 구현 결과 및 성과에 이르기까지 프로젝트 전반의 기술적 내용을 체계적으로 기술할 것입니다. 다음 장에서는 시스템의 근간을 이루는 전체 아키텍처에 대해 상세히 설명하겠습니다.

---
## 2. 프로젝트 추진 경과: 아키텍처 고도화 (Architecture Evolution)

### 2.1 MLP 설명



### 2.2 MLP 한계 분석 및 CNN 아키텍쳐 고도화

본 프로젝트 팀은 최종 시스템 구축에 앞서, 구조가 비교적 간단한 **MLP(Multi-Layer Perceptron)** 기반의 숫자 인식 가속기를 선행 개발(Prototype)하였다. 이를 통해 하드웨어 제어 로직을 검증했으나, **터치패드 입력 환경의 특수성**과 **직렬 처리 구조의 한계**를 확인하였다. 이에 우리는 이러한 한계를 하드웨어 레벨에서 극복하기 위해 **CNN(Convolutional Neural Network)** 기반의 병렬 가속기로 설계를 고도화하였다.

다음은 프로토타입(MLP)과 최종 시스템(CNN)의 기술적 차이와 고도화 전략에 대한 상세 비교 분석이다.

#### 2.2.1 데이터 처리 흐름: "저장 후 처리" vs "실시간 스트리밍" 

가장 근본적인 차이는 데이터가 입력되는 순간부터 발생한다.

* **프로토타입 (MLP): Store-and-Forward 방식**
* MLP는 입력층(Input Layer)의 784개 노드가 동시에 준비되어야 연산을 시작할 수 있다.
* 따라서 DMA가 보내주는 784개의 픽셀 데이터를 BRAM에 모두 저장할 때까지 기다려야 하며(Buffering), 이로 인해 **초기 지연(Latency)** 이 필연적으로 발생한다.


* **최종 시스템 (CNN): Streaming 방식**
* CNN 가속기는 **라인 버퍼(Line Buffer)**를 도입하여, 이미지가 전부 들어오지 않아도 3~5개의 라인만 차면 즉시 연산을 시작한다.
* 데이터가 들어오는 속도에 맞춰 물 흐르듯 연산이 진행되므로(On-the-fly), 버퍼링 지연을 최소화하고 실시간 반응성을 확보했다.


#### 2.2.2 연산 구조 및 속도: "직렬 순차 처리" vs "병렬 트리 처리"

MLP가 CNN보다 전체 연산량(MAC Operation Count) 자체는 적음에도 불구하고, 실제 추론 속도는 CNN이 훨씬 빨랐다. 이는 **하드웨어 병렬성** 의 차이에서 기인한다.

* **프로토타입 (MLP): Serial Processing**
* 단일 MAC(Multiply-Accumulate) 유닛을 사용하여, 메모리에서 가중치를 하나씩 꺼내와 순서대로 계산하는 **폰 노이만 방식**이다.
* *결과:* 50MHz의 높은 클럭에서도 1장 추론에 **약 103,991 사이클(2.08ms)**이 소요되었다.


* **최종 시스템 (CNN): Massively Parallel Processing**
* CNN은 **3채널 커널 병렬 연산()**을 통해 **한 클럭에 75개의 곱셈을 동시에 수행**한다.
* 또한, 75개의 결과를 순차적으로 더하지 않고 **이진 덧셈 트리(Adder Tree)** 구조를 적용하여 합산 지연(Critical Path)을 획기적으로 단축시켰다.
* *결과:* 10MHz의 낮은 클럭(1/5 수준)으로 동작함에도 **단 2,787 사이클(0.28ms)** 만에 추론을 완료하여, **약 7.4배의 절대 속도 향상**을 달성했다.



#### 2.2.3 양자화 전략: "수학적 정밀도" vs "시스템 인지 최적화" 

두 아키텍처는 하드웨어 자원을 사용하는 철학에서도 차이를 보였다.

* **프로토타입 (MLP): Standard Quantization**
* 범용성을 고려하여 실수(Float) 스케일 팩터를 정밀하게 계산하고, 이를 다시 정수 곱셈기(Multiplier)와 시프트 연산으로 구현하는 정석적인 방식을 택했다.
* *한계:* 정밀도는 높으나, 곱셈기(DSP) 자원을 스케일링 보정에 추가로 소모해야 했다.


* **최종 시스템 (CNN): System-Aware Quantization**
* 입력 소스가 **'터치패드(0 또는 1)'**라는 점에 착안하여, 복잡한 곱셈기 대신 단순한 **비트 시프트(`>>> 7`)** 만으로 나눗셈을 대체하는 과감한 최적화를 수행했다.
* *성과:* 불필요한 DSP 소모를 줄이고, 확보된 자원을 연산 병렬화에 집중 투자했다.

---

#### 2.2.4 터치패드 환경을 고려한 아키텍처 선정 배경

프로토타입인 MLP 모델을 검토하는 과정에서, 우리는 **터치패드 입력 환경의 특수성**과 **MLP의 구조적 특성**이 상충한다는 점을 발견했습니다. 이에 따라 실제 하드웨어 구현 단계에서는 **CNN 아키텍처로의 고도화** 가 필수적이라는 결론을 도출했습니다.

* **프로토타입(MLP)의 구조적 한계: 위치 민감성 (Shift Sensitivity)**
* MLP는 28x28 이미지를 1차원 배열(784 pixel)로 펼쳐서(Flatten) 처리하는 구조입니다. 이 과정에서 픽셀 간의 상하좌우 연결성, 즉 **공간적 정보(Spatial Information)** 가 소실됩니다.
* 이는 사용자가 터치패드의 구석에 숫자를 쓰거나 필체가 조금만 기울어져도(Shift/Rotation), 모델이 이를 전혀 다른 패턴으로 인식할 위험이 큼을 시사합니다. 따라서 손떨림이나 위치 변화가 잦은 실제 터치패드 환경에서는 안정적인 인식률을 보장하기 어렵다고 판단했습니다.


* **최종 시스템(CNN)의 해결책: 이동 불변성 (Translation Invariance)**
* 이러한 문제를 해결하기 위해, 우리는 2차원 형상 정보를 그대로 유지하며 특징을 추출하는 **CNN(합성곱 신경망)** 을 최종 아키텍처로 선정했습니다.
* CNN은 필터가 이미지를 훑으며(Sliding Window) **'선의 꺾임', '곡선', '교차점' 등의 고유 특징**을 찾아내는 방식이므로, 숫자가 패드의 어느 위치에 입력되더라도 강인하게 인식할 수 있습니다. 이는 사용자 편의성과 인식 정확도를 동시에 확보하기 위한 전략적인 선택이었습니다.


---

#### 2.2.5 총평 

| 구분 | MLP 가속기 (Prototype) | CNN 가속기 (Final) |
| --- | --- | --- |
| **주요 도전** | 복잡한 FSM 제어, 메모리 타이밍 | 라인 버퍼 관리, 데이터 흐름 동기화 |
| **클럭 주파수** | 50 MHz | **10 MHz** (안정성 중심) |
| **사이클 효율** | ~100,000 Cycles | **~2,700 Cycles** (약 37배 효율) |
| **결론** | 정밀 제어 중심의 학습용 설계 | **실전 성능 중심의 엣지 가속기 설계** |

결론적으로, 본 프로젝트는 초기 MLP 설계 경험을 바탕으로 **"단순히 돌아가는 하드웨어"** 가 아닌, **"입력 특성(터치패드)과 FPGA의 장점(병렬성)을 완벽하게 이해하고 최적화한 CNN 아키텍처"** 를 완성하였다. 이는 50MHz에서 동작하던 프로토타입보다 10MHz의 최종 시스템이 더 빠른 성능을 내는 **'아키텍처의 승리'**를 보여주는 결과이다.


## 3. 전체 시스템 구조 (Overall System Architecture)

본 시스템은 Zynq SoC의 핵심 특징인 PS(Processing System)와 PL(Programmable Logic)을 활용한 하드웨어/소프트웨어 공동 설계(Co-design)를 기반으로 합니다. PS의 ARM 코어가 전체 시스템 제어와 데이터 준비를 담당하고, PL의 FPGA 영역에는 맞춤형으로 설계된 CNN 가속기가 배치되어 실제 연산을 수행합니다. 이러한 이기종 컴퓨팅 아키텍처는 소프트웨어의 유연성과 하드웨어의 압도적인 성능을 동시에 확보하는 최적의 솔루션입니다.

### 3.1. 데이터 흐름 및 시스템 구성도

시스템의 데이터는 명확하게 정의된 파이프라인을 따라 처리됩니다. 각 단계는 AXI(Advanced eXtensible Interface) 프로토콜을 통해 유기적으로 연결되어 고속 데이터 전송을 보장합니다.

1. **입력 (Touchpad)**: 터치패드로부터 수집된 손글씨 좌표 데이터는 PS에서 28x28 이미지로 전처리된 후 DDR 메모리에 저장됩니다.
2. **데이터 전송 (AXI DMA)**: PS의 제어에 따라 AXI DMA가 DDR 메모리에서 이미지 데이터를 읽어 AXI-Stream 프로토콜 기반의 데이터 패킷으로 변환하여 PL 영역으로 고속 전송합니다.
3. **CNN 가속 (FPGA PL)**: PL에 위치한 CNN 가속기 IP는 스트림 데이터를 실시간으로 수신하여 합성곱, 풀링, 완전 연결 등 모든 추론 연산을 하드웨어적으로 처리합니다.
4. **결과 전달 및 출력 제어 (Zynq PS)**: CNN 가속기에서 최종 판별된 숫자(0~9) 결과는 PS로 전달됩니다. PS는 이 결과를 바탕으로 VGA 디스플레이에 출력할 화면을 제어합니다.
5. **시각화 (VGA IP)**: PS의 제어에 따라 AXI VDMA가 DDR 메모리의 프레임 버퍼를 읽어 VGA 컨트롤러로 전송하고, 최종적으로 모니터에 결과가 실시간으로 표시됩니다.

시스템을 구성하는 주요 IP(Intellectual Property)와 그 역할은 아래 표와 같습니다.

| 구성 요소 | 역할 | 적용 인터페이스 |
|----------|------|----------------|
| Zynq PS | 시스템 전체 제어, 데이터 전처리, IP 구동 및 결과 처리 | AXI4-Lite (제어), AXI4-MM (데이터) |
| AXI DMA | DDR 메모리와 PL IP 간의 고속 메모리-스트림 데이터 전송 | AXI4-Memory Map / AXI4-Stream |
| AXI VDMA | DDR 내 비디오 프레임 버퍼를 관리하고 VGA 컨트롤러로 전송 | AXI4-Memory Map / AXI4-Stream |
| Data FIFO | 클럭 도메인 동기화 및 데이터 속도 차이를 완충하는 버퍼 | AXI4-Stream (비동기 모드) |
| Width Converter | 데이터 버스 폭 정렬 (AXI 32bit → CNN IP 8bit) | AXI4-Stream |
| CNN 가속기 (cnn_top) | CNN 추론 연산의 핵심을 수행하는 맞춤형 하드웨어 로직 | AXI4-Stream |
| VGA Controller | AXI-Stream 데이터를 받아 VGA 타이밍 신호를 생성 및 출력 | AXI4-Stream → Physical I/O |

### 3.2 AI 모델 아키텍처 (Model Structure)

본 가속기에 구현된 CNN은 두 개의 합성곱/풀링 블록과 하나의 완전 연결 계층으로 구성된 경량화 모델입니다. 입력 이미지가 각 계층을 통과하며 데이터의 형태(Shape)가 변화하는 과정은 다음과 같습니다.

| 단계 | 레이어 (Layer) | 입력 크기 (Input Shape) | 커널/필터 정보 | 출력 크기 (Output Shape) |
| --- | --- | --- | --- | --- |
| **0** | **Input Image** | 28 x 28 x 1 | - | 28 x 28 x 1 |
| **1** | **Conv2D 1** | 28 x 28 x 1 | 5x5 Kernel, 3 Filters, ReLU | 24 x 24 x 3 |
| **2** | **MaxPooling2D 1** | 24 x 24 x 3 | 2x2 Pooling | 12 x 12 x 3 |
| **3** | **Conv2D 2** | 12 x 12 x 3 | 5x5 Kernel, 3 Filters, ReLU | 8 x 8 x 3 |
| **4** | **MaxPooling2D 2** | 8 x 8 x 3 | 2x2 Pooling | 4 x 4 x 3 |
| **5** | **Flatten** | 4 x 4 x 3 | - | 48 |
| **6** | **Dense (FC)** | 48 | 10 Units, Softmax | 10 (Class 0~9) |

---

## 4. 하드웨어 설계 (Hardware Design)

### 4.1 클럭 아키텍처 및 CDC 설계

시스템의 안정성과 성능을 최적화하기 위해, 각 IP의 요구 사항에 맞춰 3개의 독립적인 클럭 도메인을 구성했습니다.

| 클럭 도메인 | 주파수 | 적용 모듈 | 설계 근거 |
|-----------|--------|----------|----------|
| 시스템 클럭 | 50 MHz | Zynq PS, DMA, VDMA, AXI Interconnect | DDR 메모리 인터페이스와의 고속 데이터 전송 효율 최적화 |
| VGA 클럭 | 25.175 MHz | VGA Controller | VGA 640×480@60Hz 해상도의 표준 비디오 타이밍 규격 준수 |
| CNN 클럭 | 10 MHz | CNN 가속기 | 복잡한 이진 덧셈 트리(Binary Adder Tree) 파이프라인이 타이밍 위반 없이 단일 클럭 내에 완료되도록 보장하고, 설계 초기 단계에서 라우팅 복잡도를 완화하며 디버깅 용이성을 증대 |

서로 다른 클럭 속도로 동작하는 도메인 간의 데이터 전송에서 발생할 수 있는 CDC(Clock Domain Crossing) 문제는 **비동기 FIFO(Asynchronous FIFO)** 를 통해 해결했습니다. 시스템에 배치된 AXI4-Stream Data FIFO IP를 Independent Clock 모드로 설정하여, 50MHz의 시스템 클럭과 10MHz의 CNN 클럭 간의 속도 차이를 안전하게 완충하고 데이터 무결성을 완벽하게 보장합니다.

이처럼 견고하게 설계된 시스템 아키텍처는 본 프로젝트의 핵심인 CNN 가속기가 최대의 성능을 발휘할 수 있는 안정적인 기반을 제공합니다. 다음 장에서는 가속기의 내부 상세 설계에 대해 심층적으로 분석하겠습니다.

---

## 5. CNN 가속기 상세 설계 (Detailed Design)

본 장에서는 CNN 가속기의 핵심 성능을 결정하는 RTL(Verilog) 수준의 설계 최적화 기법들을 다룹니다. 단순히 기능을 구현하는 것을 넘어, Verilog 코드가 어떻게 특정 하드웨어 동작(파이프라인, 병렬 처리, 자원 재사용)을 구현했는지 코드 스니펫과 함께 심층 분석함으로써, 하드웨어 아키텍처에 대한 깊이 있는 이해를 제시합니다.

### 5.1 시스템 인지 양자화(하드웨어 친화적 양자화): 고정 스케일링 (Fixed Scaling)

FPGA의 제한된 리소스를 효율적으로 사용하기 위해 복잡한 부동소수점 연산을 배제하고 INT8 고정소수점 연산을 채택했습니다. 특히, 본 시스템의 입력 데이터가 터치패드를 통해 생성되는 0 또는 1의 이진(Binary) 데이터라는 특성에 착안하여, 하드웨어 복잡도를 획기적으로 낮추는 고정 스케일링(Fixed Scaling) 기법을 적용했습니다.

이 기법의 핵심은 signed 8-bit 정수(-128 ~ +127)의 표현 범위를 최대로 활용하는 것입니다. 스케일링 계수로 표현 가능한 가장 큰 양수인 127을 선택함으로써, 정수 변환 과정에서의 정보 손실을 최소화하고 정밀도를 극대화했습니다.

그러나 이 방식은 연산 과정에서 '스케일 폭발(Scale Explosion)' 문제를 야기합니다. 127배로 스케일링된 두 INT8 값이 곱해지면, 그 결과는 $127^2 \approx 2^{14}$배로 스케일이 커지게 됩니다. 다음 계층으로 전달하기 전, 이 값을 다시 128($2^7$)배 수준으로 낮추는 리스케일링(Re-scaling) 과정이 필수적입니다. 이때, 하드웨어에서 매우 비싼 연산인 나눗셈기를 사용하는 대신, 우측 비트 시프트(`>>> 7`) 연산으로 대체하여 DSP(Digital Signal Processing) 자원을 절약하고 타이밍 마진을 확보하는 최적화를 수행했습니다


1. **스케일 증폭:** 입력($2^7$)과 가중치($2^7$)의 곱셈 연산으로 인해 출력값의 스케일이 **$ 2^{14} $ 배**로 과도하게 커집니다 ($2^7 \times 2^7 = 2^{14}$).
2. **스케일 복원:** 다음 계층이 요구하는 $2^7$ 스케일로 정규화하기 위해, 연산 결과를 $2^7$ (128)로 나누어야 합니다 ($2^{14} \div 2^7 = 2^7$).
3. **하드웨어 최적화:** 복잡한 나눗셈기 대신 우측 비트 시프트(`>>> 7`) 연산을 사용하여 동일한 수학적 결과를 얻으면서 DSP 자원을 절약했습니다.

```verilog
// [코드 분석: conv2_calc_3.v]
// 나눗셈(/128) 대신 우측 시프트(>>>7)를 사용하여 DSP 블록을 절약하고 타이밍을 확보함
conv_out_calc <= ($signed(final_sum_s7) >>> 7) + 8'shcf;

```

마지막으로, 본 프로젝트는 최종 결과로 '어떤 숫자가 가장 높은 점수를 가졌는가(Argmax)'만을 요구합니다. 모든 출력값에 동일한 스케일이 적용되어 있으므로 값들의 순위(대소 관계)는 변하지 않습니다. 따라서 자원을 소모하는 역양자화(De-quantization) 과정을 생략하여 추가적인 하드웨어 절약 효과를 얻었습니다.


### 5.2 중앙 제어 로직 (FSM Design)

가속기 전체의 데이터 흐름과 상태를 관장하기 위해 5단계 FSM(Finite State Machine)을 설계했습니다.

* **상태 정의:** `IDLE` → `RUN_CNN` (데이터 수신) → `PADDING` (플러싱) → `WAIT_DONE` → `RESULT`

* **S_IDLE**: 시스템 초기화 및 시작(start_sw) 신호 대기
  * start_sw 입력 시 → S_RUN_CNN
* **S_RUN_CNN**: AXI-Stream 인터페이스를 통해 외부로부터 이미지 데이터를 수신
  * 마지막 데이터(tlast) 수신 시 → S_PADDING
* **S_PADDING**: 파이프라인 플러싱(Pipeline Flushing) 수행
  * 일정 시간(padding_cnt) 경과 후 → S_WAIT_DONE
* **S_WAIT_DONE**: 완전 연결 계층의 연산 완료 신호 대기
  * fc_valid 신호 감지 시 → S_RESULT
* **S_RESULT**: 최종 추론 결과를 출력하고 S_IDLE로 복귀 준비
  * start_sw 해제 시 → S_IDLE

본 가속기는 깊은 파이프라인 구조를 가지므로, 외부 데이터 입력이 끝난 후에도 내부 레지스터에는 아직 처리 중인 데이터가 남아있습니다. 이러한 데이터를 유실 없이 끝까지 처리하기 위한 파이프라인 플러싱 전략이 필수적입니다. S_PADDING 상태는 바로 이 문제를 해결하기 위해 고안되었습니다. 이 상태에서는 외부 데이터 입력을 차단하고 '0'을 강제로 주입하여, 파이프라인 내부에 남아있는 유효 데이터를 끝까지 밀어냅니다.

```verilog
// cnn_top.v
localparam S_IDLE = 3'd0, S_RUN_CNN = 3'd1, S_PADDING = 3'd2, ...;

 S_PADDING: begin
     cnn_pipeline_valid <= 1'b1;
     padding_cnt <= padding_cnt + 1;
     
     // 2000클럭 밀어내기
     if (padding_cnt > 2000) begin
          state <= S_WAIT_DONE;
          cnn_pipeline_valid <= 1'b0;
     end
 end
```

### 5.3 합성곱 연산 계층: 극단적 병렬화

합성곱 계층은 전체 연산의 병목 구간이므로 **'속도 최우선'** 설계를 적용했습니다.

#### 5.3.1 라인 버퍼 (Line Buffer)

하드웨어 Shift Register Array를 구현한 것으로, 1차원 픽셀 스트림을 입력받으면서도 매 클럭 2차원 공간 정보(5x5 윈도우)를 유지할 수 있게 해주는 스트림 처리의 핵심 기법입니다. 전체 이미지(28x28=784 픽셀)를 저장하는 대신, 연산에 필요한 단 5줄의 데이터(5x28=140 픽셀)만 저장하여 메모리 사용량을 절감했습니다.

```verilog
// conv_buf 모듈
// 매 클럭, 데이터가 한 줄씩 위로 이동하며 저장됨 (Data Reuse)
line4_regs[col_cnt] <= line3_regs[col_cnt];
line3_regs[col_cnt] <= line2_regs[col_cnt];
line2_regs[col_cnt] <= line1_regs[col_cnt];
line1_regs[col_cnt] <= data_in;
```

`conv_buf` 모듈은 스트리밍 입력 데이터로부터 2D 합성곱에 필요한 5×5 윈도우를 생성하는 라인 버퍼를 구현한다.

**동작 원리:**
1. 이미지는 1차원 스트림으로 입력되지만, 합성곱은 2D 공간 정보가 필요하다.
2. 4개의 라인 레지스터(`line1_regs` ~ `line4_regs`)에 이전 행들을 저장한다.
3. 새로운 픽셀이 입력될 때마다 5×5=25개의 픽셀을 동시에 출력(`data_out_0` ~ `data_out_24`)하여 계산 모듈로 전달한다.

이를 통해 **매 클럭마다 하나의 출력 픽셀**을 생성하는 고속 파이프라인을 구현한다.


#### 5.3.2 커널 병렬성(Kernel Parallelism)

conv_calc 설계는 **완전 병렬 커널 연산(Fully Parallel Kernel Computation)** 방식을 채택하였다.

**구현 메커니즘:**
```verilog
// conv1_calc.v 핵심 로직
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p_s0[i]) * get_w1(i);
    product2_s1[i] <= $signed(p_s0[i]) * get_w2(i);
    product3_s1[i] <= $signed(p_s0[i]) * get_w3(i);
end
```

위 코드에서 `for`문은 소프트웨어의 순차 반복이 아닌, **25개의 곱셈기를 물리적으로 병렬 배치**하는 하드웨어 기술 방식이다. 따라서 5×5 커널의 모든 원소와 입력 윈도우 간의 곱셈이 **단일 클럭 사이클 내에 동시 실행** 된다.


#### 5.3.3 이진 덧셈 트리 (Binary Adder Tree)

25개의 곱셈 결과를 순차적으로 더하는 대신, 토너먼트 방식의 트리 구조로 더하여 Critical Path를 단축시켰습니다. 이는 낮은 클럭 속도에서도 높은 처리량을 보장합니다.

```
Stage 1: 25개의 곱셈 결과
Stage 2: 13개로 축소 (2개씩 덧셈, 1개 직접 전달)
Stage 3: 7개로 축소
Stage 4: 4개로 축소
Stage 5: 2개로 축소
Stage 6: 최종 합계 1개
```

```verilog
// [코드 분석: conv1_calc.v]
// 25개의 곱셈기가 하드웨어적으로 병렬 생성되어 1클럭에 동시 연산 수행
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p_s0[i]) * get_w1(i);
end
// 이후 Stage 2~6에서 계층적 덧셈 수행 (Pipeline Registers)
for (i=0; i<12; i=i+1) sum1_s2[i] <= product1_s1[2*i] + product1_s1[2*i+1];

```

#### 5.3.4 다채널 처리(Multi-Channel Processing)

`conv2_calc_1` 모듈은 입력 채널이 3개로 증가한 2번째 합성곱 레이어를 처리한다:

```verilog
// 채널별 독립 계산
for (i = 0; i < 25; i = i + 1) begin
    product1_s1[i] <= $signed(p1_s0[i]) * get_w1(i); // 채널 1
    product2_s1[i] <= $signed(p2_s0[i]) * get_w2(i); // 채널 2
    product3_s1[i] <= $signed(p3_s0[i]) * get_w3(i); // 채널 3
end

// 최종 단계에서 채널 결과 합산
final_sum_s7 <= sum1_s6 + sum2_s6 + sum3_s6;
```

**결과:** conv2에는 3개의 calc 있기 때문에 총 225개(75×3)의 곱셈기가 병렬로 동작하며, 이는 **3D 합성곱(C_in × K_h × K_w)의 완전 병렬화** 를 의미한다.

### 5.4 완전 연결 계층: 자원 효율화

합성곱 계층이 속도 극대화를 위해 병렬 구조를 채택한 것과 달리, 완전 연결 계층(FC Layer)은 의도적인 아키텍처 트레이드오프의 결과로 자원 효율성을 극대화하기 위해 시분할(Time-Sharing) 연산 구조로 설계되었습니다. FC 계층은 합성곱 계층보다 연산량이 적으므로, 단 하나의 MAC(Multiply-Accumulate) 연산기를 FSM 제어를 통해 재사용하여 DSP와 같은 고비용 자원을 절약하고, 이를 성능이 더 중요한 합성곱 계층에 할당하는 전략적 선택을 했습니다.

또한 시스템 안정성 확보를 위해, input_buffer에 데이터를 쓰는 로직을 제어 FSM이 포함된 always 블록과 분리했습니다. 이는 리셋 신호가 제어 로직에만 영향을 미치고 데이터 저장 로직에는 영향을 주지 않도록 하여, 의도치 않은 메모리 오염(corruption)을 방지하는 효과적인 FPGA 설계 기법입니다.

```verilog
// [코드 분석: fully_connected.txt]
// 데이터 저장 로직을 별도 always 블록으로 분리하여 리셋 오동작 방지
always @(posedge clk) begin
    if (valid_in) begin
        // ... (버퍼링 로직)
        input_buffer[buffer_cnt * 3] <= $signed(data_in_1);
    end
end

```
---

## 5.5 VGA 디스플레이 컨트롤러 설계 (VGA Display Controller)

VGA 컨트롤러는 VDMA로부터 스트리밍되는 영상 데이터를 받아 표준 모니터 타이밍(Hsync, Vsync)에 맞춰 픽셀을 출력하는 역할을 합니다. 본 프로젝트에서는 **640x480 @ 60Hz** 해상도를 지원하며, AXI4-Stream 프로토콜과 VGA 타이밍 간의 비동기 문제를 해결하기 위해 **초기 프레임 동기화(Frame Synchronization) 로직**을 적용했습니다.

### 5.5.1 해상도 및 타이밍 규격

표준 VGA 신호 규격을 준수하기 위해 25MHz 픽셀 클럭을 기반으로 수평(Horizontal) 및 수직(Vertical) 타이밍 파라미터를 설정하였습니다.

```verilog
// [코드 분석: vga_controller.v]
// 640x480 @ 60Hz 표준 타이밍 파라미터 정의
localparam H_VISIBLE = 640;
localparam H_TOTAL   = 800; // Visible + Front + Sync + Back
localparam V_VISIBLE = 480;
localparam V_TOTAL   = 525;

```

### 5.5.2 프레임 동기화 및 AXI 핸드셰이킹

VDMA는 메모리 상황에 따라 버스트로 데이터를 보내지만, VGA는 엄격한 실시간 타이밍을 요구합니다. 특히 시스템 초기 구동 시, 데이터 스트림의 시작점과 VGA 카운터의 (0,0) 시점이 어긋나는 현상(Image Rolling)을 방지하기 위해 **강제 정렬 로직**을 구현했습니다.

* **동기화 전략:** 시스템 리셋 후 아직 동기화되지 않은 상태(`!system_synced`)에서 첫 번째 유효 데이터(`tvalid`)가 도착하면, 그 즉시 `h_cnt`와 `v_cnt`를 0으로 강제 초기화하여 프레임의 시작점을 맞춥니다.

```verilog
// [코드 분석: vga_controller.v]
// 시스템 켜진 후 첫 데이터(Valid)가 도착했을 때 카운터 강제 리셋 (Sync 맞춤)
if (!system_synced && s_axis_tvalid && s_axis_tready) begin
    h_cnt <= 0;          // 좌표를 무조건 0,0으로 강제 초기화
    v_cnt <= 0;
    system_synced <= 1;  // 정렬 완료 플래그 설정 (이후에는 일반 카운팅 동작)
end

```

또한, VGA가 실제로 화면을 출력하는 유효 구간(`active_area`)에서만 데이터를 요청(`tready = 1`)하는 백프레셔(Backpressure) 메커니즘을 통해 데이터 흐름을 제어합니다.

```verilog
// [코드 분석: vga_controller.v]
// 화면 출력 구간(Active Area)이거나, 초기 싱크를 맞추는 중일 때만 Ready 신호 출력
assign s_axis_tready = active_area || (!system_synced && s_axis_tvalid);

```

---

## 6. 소프트웨어 설계 및 학습 (Software & Training)

터치패드 입력 데이터는 일반적인 MNIST 이미지와 달리 **노이즈가 없고, 경계가 뚜렷하며(계단 현상), 압력 감지가 불가능한 특성**을 가집니다. 이를 모델 학습 단계에 반영하기 위해 특수한 데이터 증강(Augmentation) 함수를 적용했습니다.

### 6.1 터치패드 패턴 모방 함수

학습 데이터에 실제 하드웨어 입력 환경과 유사한 변형을 주입하여 모델의 견고성을 높였습니다. 전체 과정은 다음 3단계로 진행됩니다.

**1. 강제 이진화 (Thresholding):**

* 터치패드의 디지털 특성(누름/안 누름)을 반영하여, 픽셀 밝기가 0.3 초과인 경우 무조건 1.0으로 변환합니다.
* 이를 통해 애매한 회색조(Gray-scale)를 제거하고 **계단 현상(Aliasing)**을 인위적으로 생성합니다.

```python
# [코드 발췌] 0.3 기준 강제 이진화
binary_img = np.where(img > 0.3, 1.0, 0.0).astype(np.float32)

```

**2. 확률적 획 두께 변형 (Stochastic Morphology):**

* 사용자의 필압 차이나 터치 인식 감도를 시뮬레이션하기 위해 **60%의 확률**로 획을 변형합니다.
* **30% 확률로 팽창(Dilate)**하여 두꺼운 글씨를, **30% 확률로 침식(Erode)**하여 얇거나 끊긴 글씨를 생성하며, 나머지 40%는 원본 두께를 유지합니다.

```python
# [코드 발췌] 30% 뚱뚱하게, 30% 마르게 변형
if rand_val > 0.7:
    binary_img = cv2.dilate(binary_img, kernel, iterations=1)
elif rand_val < 0.3:
    binary_img = cv2.erode(binary_img, kernel, iterations=1)

```

**3. 노이즈 정리 및 재이진화 (Final Clean-up):**

* 형태학적 연산 과정에서 발생할 수 있는 소수점 값이나 미세한 노이즈를 제거하기 위해, 다시 한번 0.5를 기준으로 명확하게 0 또는 1로 확정합니다.

```python
# [코드 발췌] 최종 0/1 확정
final_img = np.where(binary_img > 0.5, 1.0, 0.0)

```

### 6.2 가중치 추출 및 ROM 변환

학습이 완료된 모델(`my_cnn_wild_model_hwc_v2.keras`)의 가중치(Weights)와 편향(Bias)을 추출하고, 이를 `quantize_weights` 함수를 통해 INT8로 양자화한 뒤 Verilog `function` (ROM 형태)으로 변환하여 FPGA 내부에 임베딩하였습니다.

---

## 7. 구현 결과 및 성과 (Results)

본 장에서는 설계된 CNN 가속기의 성능을 정량적으로 평가하고, 동일 알고리즘을 PC(CPU) 환경에서 실행했을 때와 비교 분석한다. 특히 절대적인 추론 시간뿐만 아니라, 클럭 사이클 당 명령어 처리 효율(IPC) 관점에서의 아키텍처적 우수성을 종합적으로 논의한다.

### 7.1 성능 측정 결과 (Performance Measurement)

FPGA 하드웨어 가속기의 성능을 검증하기 위해, 동일한 CNN 알고리즘을 PC 환경(Python Software)에서 실행한 결과와 비교하였다. 공정한 비교를 위해 PC 환경은 일반적인 임베디드 소프트웨어 실행 환경을 가정하였다.

| 구분 | PC (Software) | FPGA (Hardware) | 비고 |
| --- | --- | --- | --- |
| **플랫폼** | Intel/AMD CPU @ **3.2 GHz** | Xilinx Zynq-7000 @ **10 MHz** | 클럭 속도 **320배** 차이 |
| **구현 방식** | 순차적 소프트웨어 처리 | **완전 병렬 하드웨어 로직** | Sequential vs Parallel |
| **평균 추론 시간** | **4.3857 ms** | **0.2787 ms** (2,787 Cycles) | **약 15.7배 가속** |
| **결과** | 높은 동작 주파수에도 불구하고 SW 오버헤드로 인해 지연 발생 | 낮은 주파수이나 HW 병렬성으로 고속 처리 달성 | **FPGA 성능 우위** |

실험 결과, 3.2GHz의 고속으로 동작하는 범용 CPU보다 **10MHz의 FPGA 가속기가 약 15.7배 더 빠른** 성능을 기록하였다. 이는 동작 주파수(Frequency)의 절대적 열세(1/320 수준)를 아키텍처의 효율성으로 완벽하게 극복했음을 의미한다.

### 7.2 아키텍처 효율성 분석 (Architectural Efficiency)

단순한 시간 비교를 넘어, **'하나의 이미지를 처리하는 데 몇 번의 클럭 사이클이 필요한가?'**를 분석하면 FPGA의 구조적 우수성이 더욱 명확해진다.

* **CPU 소모 사이클 (추정):** 약 **14,034,240 Cycles**
* 계산: 
* CPU는 명령어 인출(Fetch), 디코딩, 운영체제 스케줄링 등 연산 외적인 오버헤드로 인해 막대한 사이클을 소모한다.


* **FPGA 소모 사이클 (실측):** **2,787 Cycles**
* FPGA는 오버헤드 없이 오직 연산에 필요한 사이클만 소모한다.

> **분석 결과:** 동일한 작업을 수행함에 있어 FPGA는 CPU 대비 **약 5,035배 더 효율적인 사이클(5,000x Architecture Efficiency)**로 동작함을 확인하였다. 이는 본 프로젝트가 채택한 하드웨어 가속 구조가 범용 프로세서 대비 압도적인 효율을 가짐을 증명한다.

### 7.3 성능 우위의 원인 분석

10MHz FPGA가 3.2GHz CPU를 압도할 수 있었던 핵심 원인은 다음 세 가지로 요약된다.

1. **진정한 병렬 처리 (True Spatial Parallelism):**
* CPU는 기본적으로 명령어를 순차적으로(Sequential) 처리한다.
* 반면, 본 가속기는 **Conv2 계층에서만 75개의 DSP(곱셈기)가 동시에 동작**하며, 파이프라인의 모든 단계가 멈춤 없이 흐르는 공간적 병렬성을 통해 처리 시간을 획기적으로 단축시켰다.


2. **결정론적 레이턴시 (Deterministic Latency):**
* CPU 기반 소프트웨어는 OS 인터럽트, 캐시 미스(Cache Miss), 백그라운드 프로세스 등의 영향으로 실행할 때마다 추론 시간이 미세하게 변동(Jitter)하는 문제가 있다.
* 반면, FPGA 가속기는 하드웨어 로직으로 고정되어 있어, 입력이 시작되면 **어떤 상황에서도 정확히 2,787 사이클 후에 결과가 출력됨을 보장**한다. 이러한 예측 가능성은 실시간 제어 시스템(Real-time System)에서 필수적인 장점이다.


3. **데이터 이동 최소화 및 오버헤드 제거 (Zero Overhead):**
* CPU는 데이터를 캐시와 메모리 사이에서 끊임없이 이동시켜야 하므로 '폰 노이만 병목(Von Neumann Bottleneck)' 현상이 발생한다.
* 본 가속기는 **라인 버퍼(Line Buffer)** 기술을 통해 스트리밍 데이터가 칩 내부에서 재사용되도록 설계하여 메모리 대역폭 의존도를 최소화했으며, 베어메탈(Bare-metal) 수준의 동작으로 SW 오버헤드를 원천 차단하였다.


### 7.4 리소스 사용률

효율적인 아키텍처 설계(고정 스케일링, 시분할 연산)를 통해 Zynq-7000의 제한된 자원 내에서 성공적으로 구현되었습니다.

###  FPGA 리소스 사용량 및 비율 (Resource Utilization)

| 리소스 (Resource) | 사용량 (Utilization) | 전체 가용량 (Available) | 사용률 (Utilization %) | 비고 (Description) |
| --- | --- | --- | --- | --- |
| **LUT** | 9,569 | 53,200 | **17.99%** | 로직 구현 (Logic) |
| **LUTRAM** | 413 | 17,400 | **2.37%** | 분산 메모리 (Distributed RAM) |
| **FF** | 13,726 | 106,400 | **12.90%** | 플립플롭 (Flip-Flop) |
| **BRAM** | 5 | 140 | **3.57%** | 블록 램 (Block RAM) |
| **DSP** | 181 | 220 | **82.27%** | 디지털 신호 처리 (DSP48) |
| **IO** | 32 | 125 | **25.60%** | 입출력 포트 (I/O) |
| **BUFG** | 3 | 32 | **9.38%** | 글로벌 버퍼 (Global Buffer) |

---


1. **DSP 사용률 (82.27%):**
* 합성곱(Convolution) 연산의 핵심인 **병렬 곱셈기** 구현에 DSP 자원의 약 82%를 투입하였습니다. 이는 **'속도 중심의 병렬 처리(Parallel Processing)'** 전략이 하드웨어 리소스를 적극적으로 활용하여 구현되었음을 보여줍니다.


2. **LUT 및 FF 사용률 (약 13~18%):**
* 복잡한 파이프라인 제어 로직과 상태 머신(FSM)을 구현했음에도 불구하고, 로직 자원은 비교적 여유롭게 사용되었습니다. 이는 향후 기능 확장이나 추가적인 최적화를 위한 공간이 충분함을 의미합니다.


3. **BRAM 효율성 (3.57%):**
* **라인 버퍼(Line Buffer)** 기술을 사용하여 전체 이미지를 저장하지 않고 필요한 행(Row)만 저장했기 때문에, 메모리 자원 사용량을 극도로 절약할 수 있었습니다.


### 7.5 핵심 성과 및 의의

본 프로젝트를 통해 달성한 핵심 기술적 성과는 다음과 같습니다.

1. **Full-Stack 시스템 설계 역량 확보**: AI 모델링(SW)부터 RTL 하드웨어 설계(HW), 임베디드 시스템 통합에 이르기까지 AI 가속기 개발의 전 과정을 직접 구축하며 영역 간 상호작용에 대한 깊이 있는 이해를 확보했습니다.

2. **하드웨어 최적화 사고방식 체득**: 성능이 병목인 합성곱 계층에는 **'속도 중심의 병렬 처리'** 를, 상대적으로 연산량이 적은 완전 연결 계층에는 **'자원 중심의 직렬 처리'** 를 적용하는 등, 목표에 따라 최적의 아키텍처를 선택하는 전략적 트레이드오프 분석 능력을 체득했습니다.

3. **SoC 표준 인터페이스 마스터**: 산업 표준인 AXI-Stream 프로토콜의 valid/ready 핸드셰이크 메커니즘을 완벽히 이해하고, ILA(Integrated Logic Analyzer)를 활용하여 실제 하드웨어에서 발생하는 타이밍 문제를 실시간으로 디버깅하는 실무 능력을 확보했습니다.

---

## 8. 결론 (Conclusion)

본 프로젝트는 Zynq-7000 SoC 플랫폼을 활용하여 터치패드 입력부터 CNN 연산, VGA 출력까지 이어지는 End-to-End 실시간 AI 추론 시스템을 성공적으로 구현했습니다. 

이는 10MHz의 낮은 클럭으로 동작하는 맞춤형 하드웨어 패브릭이 극단적인 병렬 처리와 사이클 효율성을 통해 수 GHz의 범용 프로세서를 능가할 수 있음을 보여주는 아키텍처 수준 최적화 의 성공적인 사례입니다.

특히, 데이터 재사용을 극대화한 라인 버퍼, Critical Path를 단축시킨 이진 덧셈 트리, 그리고 성능과 자원 간의 균형을 맞춘 병렬/직렬 아키텍처의 전략적 선택은 단순히 하나의 프로젝트 완성을 넘어, 

향후 더 복잡하고 깊은 신경망을 FPGA에 구현하기 위한 견고한 **'아키텍처적 청사진(Architectural Blueprint)'** 을 마련했다는 점에서 큰 의의를 가집니다.
